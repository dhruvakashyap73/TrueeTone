# ðŸŽ™ï¸ TrueeTone: Audio Authenticity Detection System

TrueeTone is an AI-powered audio authenticity detection system designed to identify whether an uploaded or recorded audio is **AI-generated (Fake)** or **Human-generated (Real)**. It combines multiple machine learning models, advanced feature extraction techniques, and a clean Streamlit interface to provide a reliable and user-friendly verification tool.

---

## Aim
To build an AI-driven system that detects whether an input audio file is generated by artificial intelligence or spoken by a real human voice.

---

## Objectives
- Train multiple ML models to distinguish between AI and human voice samples.
- Extract effective acoustic features using MFCC and Wav2Vec.
- Design an intuitive web interface using Streamlit and Google Colab.
- Provide prediction, visualization, and PDF reporting for analysis.

---

## Motivation
With the increasing use of synthetic speech and deepfake audio, verifying the authenticity of voice recordings has become crucial in domains like journalism, cybersecurity, and digital forensics. TrueeTone addresses this concern by offering a robust detection system powered by deep learning.

---

## Technologies Used
- **Frontend:** Streamlit, Ngrok (Colab)
- **Backend Models:** TensorFlow, PyTorch, Torchaudio, Librosa
- **Visualization:** Matplotlib, Librosa display
- **PDF Generation:** ReportLab, FPDF
- **Deployment:** Google Colab + Ngrok

---

## Machine Learning Models

### 1. Dense Neural Network (DNN) with MFCC
- Input: 40 MFCC features averaged over time.
- Architecture: 3 dense layers with ReLU and Dropout.
- Output: Softmax classification (AI or Human).
- Result: Efficient baseline model with good accuracy.

### 2. Convolutional Neural Network (CNN) with MFCC
- Input: MFCC features reshaped to 2D for Conv2D layers.
- Architecture: 2 convolutional layers + max pooling + dense layers.
- EarlyStopping and Dropout to reduce overfitting.
- Result: More effective than DNN for capturing local audio patterns.

### 3. Pretrained Wav2Vec2.0 Model (WAV2VEC2_ASR_BASE_960H)
- Framework: PyTorch with Torchaudio.
- Feature Extraction: Deep audio embeddings with entropy-based thresholding.
- Classification: Entropy > 200 â†’ Human; else AI.
- Result: Most robust and reliable predictions among all three.

---

## Methodology

1. **Dataset Preparation**  
   - Real and AI-generated audio stored in class-wise folders.
   - MFCC feature extraction and normalization.

2. **Model Training**  
   - DNN and CNN trained using TensorFlow on resampled balanced data.
   - Wav2Vec2.0 used without training â€” only for feature extraction and entropy analysis.

3. **Frontend Application**  
   - Built using Streamlit in Google Colab.
   - Supports uploading and recording audio.
   - Visualizes mel spectrograms of the input.
   - Displays results from all three models.
   - Allows users to generate and download a detailed PDF report.

4. **Report Generation**  
   - Includes audio metadata, model predictions, spectrogram plots, and final verdict.
   - Highlights the importance of audio authenticity.

---

## Results and Outcomes
- Accurate classification of real vs AI-generated audio.
- Wav2Vec model yielded the best performance with entropy-based prediction.
- Easy-to-use web interface with audio upload/recording support.
- Mel spectrogram and MFCC visualizations aid in better understanding.
- Automated PDF reports for documentation and analysis.
- Demonstrated robustness in testing across various samples.

---

## Real-World Challenges
- Variability in audio formats and quality in real-world inputs.
- Generalizing across different AI voice synthesizers.
- High computational cost for real-time Wav2Vec inference.
- Multilingual and accent variability.
- Adversarial manipulations and evasion techniques.

---

## Conclusion & Future Scope
TrueeTone successfully demonstrates that AI-generated audio can be detected using a combination of DNN, CNN, and pretrained models like Wav2Vec. As voice synthesis becomes more sophisticated, future work includes:
- Fine-tuning Wav2Vec on custom datasets.
- Real-time streaming support.
- Multilingual and multi-accent detection.
- Cloud-based scalable deployment (e.g., AWS/GCP).
- Integration with explainable AI for transparency.

---

## Dataset Reference

The dataset used in this project (containing both real and AI-generated audio samples) is large in size and is therefore hosted externally.

ðŸ“Ž Reference Dataset Link: [View the TrueeTone Dataset on Google Drive](https://drive.google.com/drive/folders/1NKPvs8Vmct0Hm2WzXPFYSNXin7n5jVVK?usp=sharing)

> Note: This link is provided for reference purposes only. 

---

## License

This project is for academic and research purposes only. Do not use in critical systems without proper validation.

---


## Additional Resources

For a complete explanation of the methodology, results, diagrams, and analysis,  
**[Click here to view the full project report](https://github.com/dhruvakashyap73/TrueeTone/blob/main/Project_Report.pdf)**

